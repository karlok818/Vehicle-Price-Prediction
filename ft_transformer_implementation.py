# FT-Transformer Implementation for Vehicle Price Regression
# Generated by GithubCopilot
# 
# This script implements FT-Transformer model that doesn't require
# traditional scaling or categorical encoding
#
# INSTALLATION:
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# pip install rtdl

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import rtdl
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import mlflow
import mlflow.sklearn
import matplotlib.pyplot as plt
import os


class FTTransformerPreprocessor:
    """
    Minimal preprocessing for FT-Transformer.
    Only drops unnecessary columns and handles missing values.
    No scaling or encoding required!
    """
    
    def __init__(self):
        self.cat_impute_values = None
        self.num_impute_values = None
        self.cat_features = None
        self.num_features = None
        self.label_encoders = {}
        
    def preprocess(self, df, is_train=True):
        """Preprocess data for FT-Transformer"""
        data = df.copy()
        
        # Convert EngineDescription to numeric
        data['EngineDescription'] = pd.to_numeric(data['EngineDescription'], errors='coerce')
        
        # Drop unnecessary columns
        cols_to_drop = [
            'Description', 'VIN', 'ModelCode', 'EngineNum', 'Series', 
            'BadgeDescription', 'Model', 'FamilyCode', 'RearTyreSize', 
            'FrontTyreSize', 'CurrentRelease', 'ImportFlag', 'GoodKM', 
            'Cylinders', 'Torque', 'WheelBase', 'SeriesModelYear', 
            'BadgeSecondaryDescription', 'BodyConfigDescription', 
            'WheelBaseConfig', 'Roofline', 'ExtraIdentification', 
            'GrossCombinationMAss', 'GrossVehicleMass', 'TareMass', 
            'PayLoad', 'PowerRPMFrom', 'TorqueRPMFrom', 'RonRating', 
            'Acceleration', 'WarrantyCustAssist', 
            'FreeScheduledService', 'FirstServiceKM', 'FirstServiceMonths', 
            'RegServiceMonths', 'AltEngEngineType', 'AltEngBatteryType', 
            'AltEngCurrentType', 'AltEngAmpHours', 'AltEngVolts', 
            'AltEngChargingMethod', 'AltEngPower', 'AltEngPowerFrom', 
            'AltEngPowerTo', 'AltEngTorque', 'AltEngTorqueFrom', 
            'AltEngTorqueTo', 'AltEngDrive', 'NormalChargeMins', 
            'QuickChargeMins', 'NormalChargeVoltage', 'QuickChargeVoltage', 
            'KMRangeElectricEng', 'ElectricEngineLocation', 'TopSpeedElectricEng', 
            'GreenhouseRating', 'AirpollutionRating', 'OverallGreenStarRating', 
            'CO2Combined', 'CO2Urban', 'CO2ExtraUrban', 'FuelUrban', 
            'FuelExtraurban', 'EmissionStandard', 'MaxEthanolBlend', 
            'AncapRating', 'VFactsPrice', 'Sold_Date', 'Compliance_Date'
        ]
        cols_to_drop = [c for c in cols_to_drop if c in data.columns]
        data.drop(columns=cols_to_drop, inplace=True)
        
        # Remove outliers (training only)
        if is_train and 'Sold_Amount' in data.columns:
            Q1 = data['Sold_Amount'].quantile(0.25)
            Q3 = data['Sold_Amount'].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            data = data[
                (data['Sold_Amount'] > 0) & 
                (data['Sold_Amount'].notna()) & 
                (data['Sold_Amount'] >= lower_bound) & 
                (data['Sold_Amount'] <= upper_bound)
            ].copy()
        
        # Identify categorical and numerical columns
        cat_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        num_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        
        # Remove target from numerical columns if present
        if 'Sold_Amount' in num_cols:
            num_cols.remove('Sold_Amount')
        
        # Impute missing values
        if is_train:
            self.cat_impute_values = {col: data[col].mode()[0] if not data[col].mode().empty else 'Unknown' 
                                     for col in cat_cols}
            self.num_impute_values = {col: data[col].median() for col in num_cols}
            self.cat_features = cat_cols
            self.num_features = num_cols
        
        # Apply imputation
        for col in cat_cols:
            data[col] = data[col].fillna(self.cat_impute_values.get(col, 'Unknown'))
        
        for col in num_cols:
            data[col] = data[col].fillna(self.num_impute_values.get(col, 0))
        
        return data
    
    def encode_features(self, X_train, X_test=None):
        """Label encode categorical features for FT-Transformer"""
        X_train_encoded = X_train.copy()
        X_test_encoded = X_test.copy() if X_test is not None else None
        
        for col in self.cat_features:
            if col in X_train_encoded.columns:
                le = LabelEncoder()
                
                # Fit on combined train+test to handle unseen categories
                if X_test is not None:
                    combined_values = pd.concat([X_train_encoded[col], X_test_encoded[col]]).astype(str).unique()
                else:
                    combined_values = X_train_encoded[col].astype(str).unique()
                
                le.fit(combined_values)
                X_train_encoded[col] = le.transform(X_train_encoded[col].astype(str))
                
                if X_test is not None:
                    X_test_encoded[col] = le.transform(X_test_encoded[col].astype(str))
                
                self.label_encoders[col] = le
        
        return X_train_encoded, X_test_encoded


class FTTransformerTrainer:
    """Train and evaluate FT-Transformer model"""
    
    def __init__(self, device='cpu'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.best_model_state = None
        
    def build_model(self, n_num_features, cat_cardinalities, config=None):
        """Build FT-Transformer model"""
        if config is None:
            config = {
                'd_token': 192,
                'n_blocks': 3,
                'attention_n_heads': 8,
                'attention_dropout': 0.2,
                'ffn_d_hidden': 256,
                'ffn_dropout': 0.1,
                'residual_dropout': 0.0,
            }
        
        self.model = rtdl.FTTransformer.make_baseline(
            n_num_features=n_num_features,
            cat_cardinalities=cat_cardinalities,
            d_token=config['d_token'],
            n_blocks=config['n_blocks'],
            attention_n_heads=config['attention_n_heads'],
            attention_dropout=config['attention_dropout'],
            ffn_d_hidden=config['ffn_d_hidden'],
            ffn_dropout=config['ffn_dropout'],
            residual_dropout=config['residual_dropout'],
            d_out=1,
        )
        
        self.model = self.model.to(self.device)
        
        n_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Model built with {n_params:,} parameters")
        
        return self.model
    
    def prepare_data_loaders(self, X_train, y_train, X_val, y_val, cat_features, num_features, batch_size=256):
        """Prepare PyTorch DataLoaders"""
        
        def to_tensors(X, y, cat_features, num_features):
            X_cat = X[cat_features].values if cat_features else np.array([]).reshape(len(X), 0)
            X_num = X[num_features].values if num_features else np.array([]).reshape(len(X), 0)
            
            X_cat_tensor = torch.tensor(X_cat, dtype=torch.long)
            X_num_tensor = torch.tensor(X_num, dtype=torch.float32)
            y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)
            
            return X_num_tensor, X_cat_tensor, y_tensor
        
        X_num_train, X_cat_train, y_train_tensor = to_tensors(X_train, y_train, cat_features, num_features)
        X_num_val, X_cat_val, y_val_tensor = to_tensors(X_val, y_val, cat_features, num_features)
        
        train_dataset = TensorDataset(X_num_train, X_cat_train, y_train_tensor)
        val_dataset = TensorDataset(X_num_val, X_cat_val, y_val_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader
    
    def train(self, train_loader, val_loader, n_epochs=50, lr=0.001, patience=10):
        """Train FT-Transformer with early stopping"""
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-5)
        criterion = nn.MSELoss()
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5, verbose=False
        )
        
        best_val_loss = float('inf')
        patience_counter = 0
        train_losses = []
        val_losses = []
        
        for epoch in range(n_epochs):
            # Training
            self.model.train()
            train_loss = 0.0
            for X_num, X_cat, y in train_loader:
                X_num, X_cat, y = X_num.to(self.device), X_cat.to(self.device), y.to(self.device)
                
                optimizer.zero_grad()
                predictions = self.model(X_num, X_cat)
                loss = criterion(predictions, y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item() * X_num.size(0)
            
            train_loss /= len(train_loader.dataset)
            
            # Validation
            self.model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for X_num, X_cat, y in val_loader:
                    X_num, X_cat, y = X_num.to(self.device), X_cat.to(self.device), y.to(self.device)
                    predictions = self.model(X_num, X_cat)
                    loss = criterion(predictions, y)
                    val_loss += loss.item() * X_num.size(0)
            
            val_loss /= len(val_loader.dataset)
            
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            
            scheduler.step(val_loss)
            
            if (epoch + 1) % 5 == 0 or epoch == 0:
                print(f"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self.best_model_state = self.model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"\nEarly stopping at epoch {epoch+1}")
                    self.model.load_state_dict(self.best_model_state)
                    break
        
        return train_losses, val_losses
    
    def evaluate(self, data_loader):
        """Evaluate model and return predictions"""
        self.model.eval()
        all_preds = []
        all_actual = []
        
        with torch.no_grad():
            for X_num, X_cat, y in data_loader:
                X_num, X_cat = X_num.to(self.device), X_cat.to(self.device)
                predictions = self.model(X_num, X_cat)
                all_preds.append(predictions.cpu().numpy())
                all_actual.append(y.numpy())
        
        y_pred = np.concatenate(all_preds, axis=0).flatten()
        y_actual = np.concatenate(all_actual, axis=0).flatten()
        
        return y_pred, y_actual
    
    def save_model(self, filepath):
        """Save model state"""
        torch.save(self.best_model_state if self.best_model_state else self.model.state_dict(), filepath)


def main():
    """Example usage - integrate this into your notebook"""
    
    # Check if PyTorch and RTDL are available
    try:
        import torch
        import rtdl
        print(f"PyTorch version: {torch.__version__}")
        print(f"CUDA available: {torch.cuda.is_available()}")
    except ImportError as e:
        print(f"Error: {e}")
        print("Please install: pip install torch rtdl")
        return
    
    print("\n" + "="*60)
    print("FT-Transformer Implementation Ready!")
    print("="*60)
    print("\nUsage in your notebook:")
    print("1. Load your df_train and df_test")
    print("2. Create preprocessor: preprocessor = FTTransformerPreprocessor()")
    print("3. Preprocess data: train_ft = preprocessor.preprocess(df_train, is_train=True)")
    print("4. Encode features: X_train, X_test = preprocessor.encode_features(...)")
    print("5. Create trainer: trainer = FTTransformerTrainer()")
    print("6. Build model: trainer.build_model(...)")
    print("7. Train: trainer.train(...)")
    print("8. Evaluate: trainer.evaluate(...)")
    print("="*60)


if __name__ == "__main__":
    main()
