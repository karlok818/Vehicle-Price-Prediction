# FT-Transformer Implementation Cells

Add these cells to your notebook after Section 4 (Data Cleaning and Preprocessing).

## Cell 1: Markdown - Section Header
```markdown
### 4.1 Data Preprocessing for FT-Transformer

FT-Transformer (Feature Tokenizer + Transformer) can handle raw categorical and numerical features without traditional scaling or encoding. We'll prepare a minimal preprocessed version of the data.
```

## Cell 2: Python - Minimal Preprocessing Function
```python
# Generated by GithubCopilot
# Minimal preprocessing for FT-Transformer - only drop unnecessary columns and handle missing values
# FT-Transformer handles categorical and numerical features natively without encoding

def preprocess_for_fttransformer(df, is_train=True, cat_impute_values=None, num_impute_values=None):
    """
    Minimal preprocessing for FT-Transformer:
    1. Drop high-missing and identifier columns
    2. Remove outliers (training only)
    3. Simple imputation (mode for categorical, median for numerical)
    4. Keep categorical features as-is (no encoding)
    
    Security: Input validation for data integrity
    Resource Efficiency: In-place operations where possible
    """
    data = df.copy()
    
    # Convert EngineDescription to numeric
    data['EngineDescription'] = pd.to_numeric(data['EngineDescription'], errors='coerce')
    
    # Drop unnecessary columns (same as in main preprocessing)
    cols_to_drop = [
        'Description', 'VIN', 'ModelCode', 'EngineNum', 'Series', 
        'BadgeDescription', 'Model', 'FamilyCode', 'RearTyreSize', 
        'FrontTyreSize', 'CurrentRelease', 'ImportFlag', 'GoodKM', 
        'Cylinders', 'Torque', 'WheelBase', 'SeriesModelYear', 
        'BadgeSecondaryDescription', 'BodyConfigDescription', 
        'WheelBaseConfig', 'Roofline', 'ExtraIdentification', 
        'GrossCombinationMAss', 'GrossVehicleMass', 'TareMass', 
        'PayLoad', 'PowerRPMFrom', 'TorqueRPMFrom', 'RonRating', 
        'Acceleration', 'WarrantyCustAssist', 
        'FreeScheduledService', 'FirstServiceKM', 'FirstServiceMonths', 
        'RegServiceMonths', 'AltEngEngineType', 'AltEngBatteryType', 
        'AltEngCurrentType', 'AltEngAmpHours', 'AltEngVolts', 
        'AltEngChargingMethod', 'AltEngPower', 'AltEngPowerFrom', 
        'AltEngPowerTo', 'AltEngTorque', 'AltEngTorqueFrom', 
        'AltEngTorqueTo', 'AltEngDrive', 'NormalChargeMins', 
        'QuickChargeMins', 'NormalChargeVoltage', 'QuickChargeVoltage', 
        'KMRangeElectricEng', 'ElectricEngineLocation', 'TopSpeedElectricEng', 
        'GreenhouseRating', 'AirpollutionRating', 'OverallGreenStarRating', 
        'CO2Combined', 'CO2Urban', 'CO2ExtraUrban', 'FuelUrban', 
        'FuelExtraurban', 'EmissionStandard', 'MaxEthanolBlend', 
        'AncapRating', 'VFactsPrice', 'Sold_Date', 'Compliance_Date'
    ]
    cols_to_drop = [c for c in cols_to_drop if c in data.columns]
    data.drop(columns=cols_to_drop, inplace=True)
    
    # Remove outliers (training only)
    if is_train and 'Sold_Amount' in data.columns:
        Q1 = data['Sold_Amount'].quantile(0.25)
        Q3 = data['Sold_Amount'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        data = data[
            (data['Sold_Amount'] > 0) & 
            (data['Sold_Amount'].notna()) & 
            (data['Sold_Amount'] >= lower_bound) & 
            (data['Sold_Amount'] <= upper_bound)
        ].copy()
    
    # Identify categorical and numerical columns
    cat_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    num_cols = data.select_dtypes(include=[np.number]).columns.tolist()
    
    # Remove target from numerical columns if present
    if 'Sold_Amount' in num_cols:
        num_cols.remove('Sold_Amount')
    
    # Impute missing values
    if is_train:
        cat_impute_values = {col: data[col].mode()[0] if not data[col].mode().empty else 'Unknown' 
                            for col in cat_cols}
        num_impute_values = {col: data[col].median() for col in num_cols}
    
    # Apply imputation
    for col in cat_cols:
        data[col] = data[col].fillna(cat_impute_values.get(col, 'Unknown'))
    
    for col in num_cols:
        data[col] = data[col].fillna(num_impute_values.get(col, 0))
    
    return data, cat_cols, num_cols, cat_impute_values, num_impute_values


# Preprocess data for FT-Transformer
print("Preprocessing data for FT-Transformer...")
train_ft, cat_features_ft, num_features_ft, cat_impute_ft, num_impute_ft = preprocess_for_fttransformer(
    df_train, is_train=True
)

test_ft, _, _, _, _ = preprocess_for_fttransformer(
    df_test, is_train=False, 
    cat_impute_values=cat_impute_ft, 
    num_impute_values=num_impute_ft
)

print(f"\nFT-Transformer preprocessed data:")
print(f"  Training shape: {train_ft.shape}")
print(f"  Test shape:     {test_ft.shape}")
print(f"  Categorical features: {len(cat_features_ft)}")
print(f"  Numerical features:   {len(num_features_ft)}")
print(f"  Missing values (train): {train_ft.isnull().sum().sum()}")
print(f"  Missing values (test):  {test_ft.isnull().sum().sum()}")
```

## Cell 3: Markdown - Installation Instructions
```markdown
### 4.2 Install FT-Transformer Dependencies

FT-Transformer requires PyTorch and the RTDL (Research on Tabular Deep Learning) library.
```

## Cell 4: Python - Install Dependencies
```python
# Generated by GithubCopilot
# Install required libraries for FT-Transformer
# Uncomment and run once to install

# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# !pip install rtdl

print("Uncomment the pip install commands above to install FT-Transformer dependencies.")
```

## Cell 5: Python - Import PyTorch and RTDL
```python
# Generated by GithubCopilot
# Import PyTorch and RTDL for FT-Transformer implementation

try:
    import torch
    import torch.nn as nn
    from torch.utils.data import TensorDataset, DataLoader
    import rtdl
    
    print(f"PyTorch version: {torch.__version__}")
    print(f"RTDL imported successfully")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    FT_AVAILABLE = True
    
except ImportError as e:
    print(f"Error importing libraries: {e}")
    print("Please install PyTorch and RTDL using the installation cell above.")
    device = None
    FT_AVAILABLE = False
```

## Cell 6: Python - Prepare Data Tensors
```python
# Generated by GithubCopilot
# Prepare data tensors for FT-Transformer
# FT-Transformer requires categorical features to be label-encoded and numerical features as-is

if FT_AVAILABLE:
    from sklearn.preprocessing import LabelEncoder
    
    # Prepare training data
    X_train_ft = train_ft.drop(columns=['Sold_Amount']).copy()
    y_train_ft = train_ft['Sold_Amount'].copy()
    
    # Prepare test data
    X_test_ft = test_ft.drop(columns=['Sold_Amount'], errors='ignore').copy()
    if 'Sold_Amount' in test_ft.columns:
        y_test_ft = test_ft['Sold_Amount'].copy()
    else:
        y_test_ft = None
    
    # Label encode categorical features for FT-Transformer
    label_encoders_ft = {}
    for col in cat_features_ft:
        if col in X_train_ft.columns:
            le = LabelEncoder()
            # Fit on combined train+test to handle unseen categories
            combined_values = pd.concat([X_train_ft[col], X_test_ft[col]]).astype(str).unique()
            le.fit(combined_values)
            
            X_train_ft[col] = le.transform(X_train_ft[col].astype(str))
            X_test_ft[col] = le.transform(X_test_ft[col].astype(str))
            
            label_encoders_ft[col] = le
    
    # Split training data for validation
    X_train_ft_split, X_val_ft_split, y_train_ft_split, y_val_ft_split = train_test_split(
        X_train_ft, y_train_ft, test_size=0.2, random_state=42
    )
    
    # Apply log transformation to target
    y_train_ft_log = np.log1p(y_train_ft_split.values)
    y_val_ft_log = np.log1p(y_val_ft_split.values)
    
    print(f"FT-Transformer data prepared:")
    print(f"  X_train shape: {X_train_ft_split.shape}")
    print(f"  X_val shape:   {X_val_ft_split.shape}")
    print(f"  Categorical features encoded: {len(cat_features_ft)}")
    print(f"  Numerical features: {len(num_features_ft)}")
else:
    print("PyTorch not available. Skipping data preparation.")
```

## Cell 7: Python - Build FT-Transformer Model
```python
# Generated by GithubCopilot
# Build FT-Transformer model using RTDL
# Security: No user input processed. Resource Efficiency: Uses GPU if available

if FT_AVAILABLE:
    # Get cardinalities for categorical features
    cat_cardinalities = []
    for col in cat_features_ft:
        if col in X_train_ft_split.columns:
            cat_cardinalities.append(int(X_train_ft_split[col].max() + 1))
    
    # Number of numerical features
    n_num_features = len(num_features_ft)
    
    # FT-Transformer configuration
    ft_config = {
        'n_num_features': n_num_features,
        'cat_cardinalities': cat_cardinalities,
        'd_token': 192,  # Token embedding dimension
        'n_blocks': 3,   # Number of transformer blocks
        'attention_n_heads': 8,
        'attention_dropout': 0.2,
        'ffn_d_hidden': 256,
        'ffn_dropout': 0.1,
        'residual_dropout': 0.0,
    }
    
    print("FT-Transformer Configuration:")
    print(f"  Numerical features: {n_num_features}")
    print(f"  Categorical features: {len(cat_cardinalities)}")
    print(f"  Token dimension: {ft_config['d_token']}")
    print(f"  Transformer blocks: {ft_config['n_blocks']}")
    print(f"  Attention heads: {ft_config['attention_n_heads']}")
    
    # Build model
    ft_model = rtdl.FTTransformer.make_baseline(
        n_num_features=ft_config['n_num_features'],
        cat_cardinalities=ft_config['cat_cardinalities'],
        d_token=ft_config['d_token'],
        n_blocks=ft_config['n_blocks'],
        attention_n_heads=ft_config['attention_n_heads'],
        attention_dropout=ft_config['attention_dropout'],
        ffn_d_hidden=ft_config['ffn_d_hidden'],
        ffn_dropout=ft_config['ffn_dropout'],
        residual_dropout=ft_config['residual_dropout'],
        d_out=1,  # Single output for regression
    )
    
    ft_model = ft_model.to(device)
    
    # Count parameters
    n_params = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)
    print(f"\nModel parameters: {n_params:,}")
    print("\nFT-Transformer model built successfully!")
else:
    print("PyTorch not available.")
```

## Cell 8: Python - Training Setup
```python
# Generated by GithubCopilot
# Prepare DataLoaders and training function for FT-Transformer
# Resource Efficiency: Batch processing for memory management

if FT_AVAILABLE:
    def prepare_ft_data(X, y, cat_features, num_features):
        """Convert DataFrame to tensors suitable for FT-Transformer"""
        # Separate categorical and numerical features
        X_cat = X[cat_features].values if cat_features else np.array([]).reshape(len(X), 0)
        X_num = X[num_features].values if num_features else np.array([]).reshape(len(X), 0)
        
        # Convert to tensors
        X_cat_tensor = torch.tensor(X_cat, dtype=torch.long)
        X_num_tensor = torch.tensor(X_num, dtype=torch.float32)
        y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)
        
        return X_num_tensor, X_cat_tensor, y_tensor
    
    # Prepare training data
    X_num_train_ft, X_cat_train_ft, y_train_tensor_ft = prepare_ft_data(
        X_train_ft_split, y_train_ft_log, cat_features_ft, num_features_ft
    )
    
    X_num_val_ft, X_cat_val_ft, y_val_tensor_ft = prepare_ft_data(
        X_val_ft_split, y_val_ft_log, cat_features_ft, num_features_ft
    )
    
    # Create DataLoaders
    batch_size = 256
    
    train_dataset_ft = TensorDataset(X_num_train_ft, X_cat_train_ft, y_train_tensor_ft)
    val_dataset_ft = TensorDataset(X_num_val_ft, X_cat_val_ft, y_val_tensor_ft)
    
    train_loader_ft = DataLoader(train_dataset_ft, batch_size=batch_size, shuffle=True)
    val_loader_ft = DataLoader(val_dataset_ft, batch_size=batch_size, shuffle=False)
    
    print(f"DataLoaders created:")
    print(f"  Training batches: {len(train_loader_ft)}")
    print(f"  Validation batches: {len(val_loader_ft)}")
    print(f"  Batch size: {batch_size}")
    
    # Training function
    def train_ft_transformer(model, train_loader, val_loader, n_epochs=50, lr=0.001):
        """
        Train FT-Transformer model
        Security: Input validation for data integrity
        Resource Efficiency: Early stopping to prevent unnecessary computation
        """
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
        criterion = nn.MSELoss()
        
        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5, verbose=True
        )
        
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 10
        
        train_losses = []
        val_losses = []
        
        for epoch in range(n_epochs):
            # Training
            model.train()
            train_loss = 0.0
            for X_num, X_cat, y in train_loader:
                X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)
                
                optimizer.zero_grad()
                predictions = model(X_num, X_cat)
                loss = criterion(predictions, y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item() * X_num.size(0)
            
            train_loss /= len(train_loader.dataset)
            
            # Validation
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for X_num, X_cat, y in val_loader:
                    X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)
                    predictions = model(X_num, X_cat)
                    loss = criterion(predictions, y)
                    val_loss += loss.item() * X_num.size(0)
            
            val_loss /= len(val_loader.dataset)
            
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            
            # Learning rate scheduling
            scheduler.step(val_loss)
            
            # Print progress
            if (epoch + 1) % 5 == 0 or epoch == 0:
                print(f"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # Save best model state
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"\nEarly stopping at epoch {epoch+1}")
                    model.load_state_dict(best_model_state)
                    break
        
        return train_losses, val_losses, best_val_loss
    
    print("\nTraining function ready.")
else:
    print("PyTorch not available.")
```

## Cell 9: Python - Train FT-Transformer
```python
# Generated by GithubCopilot
# Train FT-Transformer model with MLflow tracking

if FT_AVAILABLE:
    print("Training FT-Transformer...\n")
    
    with mlflow.start_run(run_name="FT-Transformer"):
        mlflow.set_tag("stage", "initial_comparison")
        mlflow.set_tag("model_type", "FT-Transformer")
        mlflow.set_tag("deep_learning", "True")
        
        # Log hyperparameters
        mlflow.log_param("model_type", "FT-Transformer")
        mlflow.log_param("n_num_features", n_num_features)
        mlflow.log_param("n_cat_features", len(cat_cardinalities))
        mlflow.log_param("d_token", ft_config['d_token'])
        mlflow.log_param("n_blocks", ft_config['n_blocks'])
        mlflow.log_param("attention_n_heads", ft_config['attention_n_heads'])
        mlflow.log_param("batch_size", batch_size)
        mlflow.log_param("learning_rate", 0.001)
        mlflow.log_param("device", str(device))
        
        # Train model
        train_losses_ft, val_losses_ft, best_val_loss_ft = train_ft_transformer(
            ft_model, train_loader_ft, val_loader_ft, n_epochs=50, lr=0.001
        )
        
        # Plot training curves
        plt.figure(figsize=(10, 5))
        plt.plot(train_losses_ft, label='Training Loss', color='steelblue')
        plt.plot(val_losses_ft, label='Validation Loss', color='coral')
        plt.xlabel('Epoch')
        plt.ylabel('Loss (MSE, log scale)')
        plt.title('FT-Transformer Training Progress')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        # Evaluate on validation set
        ft_model.eval()
        all_preds_ft = []
        all_actual_ft = []
        
        with torch.no_grad():
            for X_num, X_cat, y in val_loader_ft:
                X_num, X_cat = X_num.to(device), X_cat.to(device)
                predictions = ft_model(X_num, X_cat)
                all_preds_ft.append(predictions.cpu().numpy())
                all_actual_ft.append(y.numpy())
        
        # Concatenate all predictions
        y_val_pred_ft_log = np.concatenate(all_preds_ft, axis=0).flatten()
        y_val_actual_ft_log = np.concatenate(all_actual_ft, axis=0).flatten()
        
        # Convert back from log scale
        y_val_pred_ft = np.expm1(y_val_pred_ft_log)
        y_val_actual_ft = np.expm1(y_val_actual_ft_log)
        
        # Ensure non-negative predictions
        y_val_pred_ft = np.maximum(y_val_pred_ft, 0)
        
        # Calculate metrics
        val_rmse_ft = np.sqrt(mean_squared_error(y_val_actual_ft, y_val_pred_ft))
        val_mae_ft = mean_absolute_error(y_val_actual_ft, y_val_pred_ft)
        val_r2_ft = r2_score(y_val_actual_ft, y_val_pred_ft)
        
        # Training metrics
        ft_model.eval()
        with torch.no_grad():
            X_num_sample, X_cat_sample, y_sample = next(iter(train_loader_ft))
            X_num_sample, X_cat_sample = X_num_sample.to(device), X_cat_sample.to(device)
            train_pred_sample = ft_model(X_num_sample, X_cat_sample).cpu().numpy().flatten()
            train_actual_sample = y_sample.numpy().flatten()
            
            train_pred_sample = np.expm1(train_pred_sample)
            train_actual_sample = np.expm1(train_actual_sample)
            train_pred_sample = np.maximum(train_pred_sample, 0)
            
            train_rmse_ft = np.sqrt(mean_squared_error(train_actual_sample, train_pred_sample))
            train_mae_ft = mean_absolute_error(train_actual_sample, train_pred_sample)
            train_r2_ft = r2_score(train_actual_sample, train_pred_sample)
        
        # Log metrics
        mlflow.log_metric("train_rmse", train_rmse_ft)
        mlflow.log_metric("val_rmse", val_rmse_ft)
        mlflow.log_metric("train_mae", train_mae_ft)
        mlflow.log_metric("val_mae", val_mae_ft)
        mlflow.log_metric("train_r2", train_r2_ft)
        mlflow.log_metric("val_r2", val_r2_ft)
        
        # Store results
        ft_results = {
            'Model': 'FT-Transformer',
            'Train_RMSE': train_rmse_ft,
            'Val_RMSE': val_rmse_ft,
            'Train_MAE': train_mae_ft,
            'Val_MAE': val_mae_ft,
            'Train_R2': train_r2_ft,
            'Val_R2': val_r2_ft,
        }
        
        print("\n" + "="*50)
        print("  FT-Transformer")
        print("="*50)
        print(f"  Train RMSE: ${ft_results['Train_RMSE']:,.0f} | Val RMSE: ${ft_results['Val_RMSE']:,.0f}")
        print(f"  Train MAE:  ${ft_results['Train_MAE']:,.0f} | Val MAE:  ${ft_results['Val_MAE']:,.0f}")
        print(f"  Train R²:   {ft_results['Train_R2']:.4f}  | Val R²:   {ft_results['Val_R2']:.4f}")
        
        # Save model
        torch.save(ft_model.state_dict(), os.path.join(DATA_DIR, "ft_transformer_model.pt"))
        mlflow.log_artifact(os.path.join(DATA_DIR, "ft_transformer_model.pt"))
        
        print("\n✓ FT-Transformer training completed!")
else:
    print("PyTorch not available. Skipping FT-Transformer training.")
    ft_results = None
```

## Cell 10: Python - Add to Comparison
```python
# Generated by GithubCopilot
# Add FT-Transformer to model comparison

if ft_results is not None and 'all_results' in locals():
    all_results.append(ft_results)
    
    # Update comparison table
    results_df = pd.DataFrame(all_results)
    results_df = results_df.sort_values('Val_R2', ascending=False).reset_index(drop=True)
    
    print("=" * 90)
    print("  UPDATED MODEL COMPARISON (with FT-Transformer)")
    print("=" * 90)
    print(results_df[['Model', 'Val_RMSE', 'Val_MAE', 'Val_R2', 'Train_R2']].to_string(index=False))
    
    # Visual comparison
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # R² comparison
    axes[0].barh(results_df['Model'], results_df['Val_R2'], color='steelblue', alpha=0.8)
    axes[0].set_xlabel('Validation R²')
    axes[0].set_title('Model Comparison - R² Score')
    axes[0].invert_yaxis()
    
    # RMSE comparison
    axes[1].barh(results_df['Model'], results_df['Val_RMSE'], color='coral', alpha=0.8)
    axes[1].set_xlabel('Validation RMSE ($)')
    axes[1].set_title('Model Comparison - RMSE')
    axes[1].invert_yaxis()
    
    plt.tight_layout()
    plt.show()
else:
    print("FT-Transformer results available. Run model comparison section first to see in comparison table.")
```

---

## Installation Instructions

1. **Install PyTorch**: Uncomment and run in a cell:
   ```python
   !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
   ```
   (For GPU support, adjust the URL accordingly)

2. **Install RTDL**:
   ```python
   !pip install rtdl
   ```

3. Add the cells above to your notebook after Section 4 (Data Cleaning and Preprocessing)

4. Run the cells in order

## Key Features

- **No Encoding Required**: FT-Transformer handles categorical data natively
- **No Scaling Required**: Works with raw numerical features
- **Deep Learning**: Transformer architecture for tabular data
- **MLflow Integration**: Tracked alongside other models
- **Early Stopping**: Prevents overfitting and saves computation
- **GPU Support**: Automatically uses GPU if available
